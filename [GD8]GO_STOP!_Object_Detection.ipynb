{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 8. GO/STOP! - Object Detection 시스템 만들기\n",
        "## 자율주행 보조장치\n",
        "### (1) KITTI 데이터셋\n",
        "\n",
        "**자율주행 보조장치 object detection 요구사항**\n",
        "\n",
        "- 1) 사람이 카메라에 감지되면 정지\n",
        "- 2) 차량이 일정 크기 이상으로 감지되면 정지\n",
        "\n",
        "\n",
        "![img](https://d3s0tskafalll9.cloudfront.net/media/original_images/GC-4-P-passat_sensors.jpg)\n",
        "http://www.cvlibs.net/datasets/kitti/\n",
        "\n",
        "\n",
        "이번 시간에는 tensorflow_datasets에서 제공하는 KITTI 데이터셋을 사용해보겠습니다. KITTI 데이터셋은 자율주행을 위한 데이터셋으로 2D object detection 뿐만 아니라 깊이까지 포함한 3D object detection 라벨 등을 제공하고 있습니다.\n",
        "\n",
        "- [cvlibs에서 제공하는 KITTI 데이터셋](http://www.cvlibs.net/datasets/kitti/)\n",
        "\n",
        "필요한 라이브러리를 불러 오겠습니다.\n",
        "- KITTI 데이터셋을 다운로드"
      ],
      "metadata": {
        "id": "FqA-0GFdnCzz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FU4OeIQ-S6Ok"
      },
      "outputs": [],
      "source": [
        "import os, copy\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "DATA_PATH = '/content/drive/MyDrive/kitti'\n",
        "\n",
        "(ds_train, ds_test), ds_info = tfds.load(\n",
        "    'kitti',\n",
        "    data_dir=DATA_PATH,\n",
        "    split=['train', 'test'],\n",
        "    shuffle_files=True,\n",
        "    with_info=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "다운로드한 KITTI 데이터셋을 tfds.show_examples를 통해 보도록 합시다. 우리가 일반적으로 보는 사진보다 광각으로 촬영되어 다양한 각도의 물체를 확인할 수 있습니다."
      ],
      "metadata": {
        "id": "DgMIPTuOngli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Helper libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import copy\n",
        "import cv2\n",
        "from PIL import Image, ImageDraw"
      ],
      "metadata": {
        "id": "X3smwW5BfJcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = tfds.show_examples(ds_train, ds_info)"
      ],
      "metadata": {
        "id": "vox03QcRniBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터 다운로드 시 담아둔 ds_info에서는 불러온 데이터셋의 정보를 확인할 수 있습니다.\n",
        "오늘 사용할 데이터셋은\n",
        "- 6,347개의 학습 데이터(training data)\n",
        "- 711개의 평가용 데이터(test data)\n",
        "- 423개의 검증용 데이터(validation data)로 구성되어 있습니다.\n",
        "- 라벨에는 alpha, bbox, dimensions, location, occluded, rotation_y, truncated 등의 정보가 있습니다."
      ],
      "metadata": {
        "id": "Dq-qnDaVnj9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_info"
      ],
      "metadata": {
        "id": "dj5-5bMYnmUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (2) 데이터 직접 확인하기\n",
        "이번에는 데이터셋을 직접 확인하는 시간을 갖도록 하겠습니다. ds_train.take(1)을 통해서 데이터셋을 하나씩 뽑아볼 수 있는 sample을 얻을 수 있습니다. 이렇게 뽑은 데이터에는 image 등의 정보가 포함되어 있습니다.\n",
        "\n",
        "눈으로 확인해서 학습에 사용할 데이터를 직접 이해해 봅시다."
      ],
      "metadata": {
        "id": "22xyd_gRoDFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = ds_train.take(1)\n",
        "\n",
        "for example in sample:  \n",
        "    print('------Example------')\n",
        "    print(list(example.keys()))\n",
        "    image = example[\"image\"]\n",
        "    filename = example[\"image/file_name\"].numpy().decode('utf-8')\n",
        "    objects = example[\"objects\"]\n",
        "\n",
        "print('------objects------')\n",
        "print(objects)\n",
        "\n",
        "img = Image.fromarray(image.numpy())\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KkopNts2oGa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이미지와 라벨을 얻는 방법을 알게 되었습니다. 그렇다면 이렇게 얻은 이미지의 바운딩 박스(bounding box, bbox)를 확인하기 위해서는 어떻게 해야 할까요?\n",
        "\n",
        "아래는 KITTI에서 제공하는 데이터셋에 대한 설명입니다.\n",
        "\n",
        "```python\n",
        "데이터셋 이해를 위한 예시\n",
        "Values    Name      Description\n",
        "----------------------------------------------------------------------------\n",
        "   1    type         Describes the type of object: 'Car', 'Van', 'Truck',\n",
        "                     'Pedestrian', 'Person_sitting', 'Cyclist', 'Tram',\n",
        "                     'Misc' or 'DontCare'\n",
        "   1    truncated    Float from 0 (non-truncated) to 1 (truncated), where\n",
        "                     truncated refers to the object leaving image boundaries\n",
        "   1    occluded     Integer (0,1,2,3) indicating occlusion state:\n",
        "                     0 = fully visible, 1 = partly occluded\n",
        "                     2 = largely occluded, 3 = unknown\n",
        "   1    alpha        Observation angle of object, ranging [-pi..pi]\n",
        "   4    bbox         2D bounding box of object in the image (0-based index):\n",
        "                     contains left, top, right, bottom pixel coordinates\n",
        "   3    dimensions   3D object dimensions: height, width, length (in meters)\n",
        "   3    location     3D object location x,y,z in camera coordinates (in meters)\n",
        "   1    rotation_y   Rotation ry around Y-axis in camera coordinates [-pi..pi]\n",
        "   1    score        Only for results: Float, indicating confidence in\n",
        "                     detection, needed for p/r curves, higher is better.\n",
        "```\n",
        "\n",
        "![img](https://d3s0tskafalll9.cloudfront.net/media/images/GC-4-P-input.max-800x600.png)\n",
        "[KITTI 원본이미지 예시]\n",
        "\n",
        "![img](https://d3s0tskafalll9.cloudfront.net/media/images/GC-4-P-sample.max-800x600.png)\n",
        "[KITTI 이미지 바운딩 박스 시각화 예시]\n",
        "\n",
        "위 설명과 예시 이미지를 참고하셔서 이미지 위에 바운딩 박스를 그려서 시각화해 보세요!\n",
        "\n",
        "[Pillow 라이브러리의 ImageDraw 모듈](https://pillow.readthedocs.io/en/stable/reference/ImageDraw.html)을 참고하세요."
      ],
      "metadata": {
        "id": "pgBOiTuXoJu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지 위에 바운딩 박스를 그려 화면에 표시해 주세요.\n",
        "def visualize_bbox(input_image, object_bbox):\n",
        "    input_image = copy.deepcopy(input_image)\n",
        "    draw = ImageDraw.Draw(input_image)\n",
        "    \n",
        "    # 바운딩 박스 좌표(x_min, x_max, y_min, y_max) 구하기\n",
        "    width, height = img.size\n",
        "    x_min = object_bbox[:,1] * width\n",
        "    x_max = object_bbox[:,3] * width\n",
        "    y_min = height - object_bbox[:,0] * height\n",
        "    y_max = height - object_bbox[:,2] * height\n",
        "    \n",
        "    # 바운딩 박스 그리기\n",
        "    rects = np.stack([x_min, y_min, x_max, y_max], axis=1)\n",
        "    for _rect in rects:\n",
        "        draw.rectangle(_rect, outline=(255,0,0), width=2)\n",
        "\n",
        "    return input_image\n",
        "\n",
        "visualize_bbox(img, objects['bbox'].numpy())"
      ],
      "metadata": {
        "id": "-ih2icUroJPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RetinaNet\n",
        "\n",
        "- [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002)\n",
        "\n",
        "RetinaNet은 **Focal Loss for Dense Object Detection** 논문을 통해 공개된 detection 모델입니다.\n",
        "\n",
        "1-stage detector 모델인 YOLO와 SSD는 2-stage detector인 Faster-RCNN 등보다 속도는 빠르지만 성능이 낮은 문제를 가지고 있었습니다. 이를 해결하기 위해서 RetinaNet에서는 **focal loss**와 **FPN(Feature Pyramid Network)** 를 적용한 네트워크를 사용합니다."
      ],
      "metadata": {
        "id": "nqFqQFyhoxF_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Focal Loss\n",
        "> 물체를 배경보다 더 잘 학습하자 == 물체인 경우 Loss를 작게 만들자\n",
        "\n",
        "![img](https://d3s0tskafalll9.cloudfront.net/media/images/GC-4-P-car.max-800x600.png)\n",
        "https://www.jeremyjordan.me/object-detection-one-stage/\n",
        "\n",
        "Focal loss는 기존의 1-stage detection 모델들(YOLO, SSD)이 물체 전경과 배경을 담고 있는 모든 그리드(grid)에 대해 한 번에 학습됨으로 인해서 생기는 클래스 간의 불균형을 해결하고자 도입되었습니다. 여기서 그리드(grid)와 픽셀(pixel)이 혼란스러울 수 있겠는데, 위 그림 왼쪽 7x7 feature level에서는 한 픽셀이고, 오른쪽의 image level(자동차 사진)에서 보이는 그리드는 각 픽셀의 receptive field입니다.\n",
        "\n",
        "그림에서 보이는 것처럼 우리가 사용하는 이미지는 물체보다는 많은 배경을 학습하게 됩니다. 논문에서는 이를 해결하기 위해서 Loss를 개선하여 정확도를 높였습니다.\n",
        "\n",
        "![img](https://d3s0tskafalll9.cloudfront.net/media/original_images/focal_loss.png)\n",
        "https://arxiv.org/abs/1708.02002\n",
        "\n",
        "\n",
        "Focal loss는 우리가 많이 사용해왔던 교차 엔트로피를 기반으로 만들어졌습니다. 위 그림을 보면 Focal loss는 그저 교차 엔트로피 $CE(p_t)$의 앞단에 간단히 $(1-p_t)^\\gamma$\n",
        "라는 modulating factor를 붙여주었습니다.\n",
        "\n",
        "교차 엔트로피의 개형을 보면 ground truth class에 대한 확률이 높으면 잘 분류된 것으로 판단되므로 손실이 줄어드는 것을 볼 수 있습니다. 하지만 확률이 1에 매우 가깝지 않은 이상 상당히 큰 손실로 이어지는데요.\n",
        "\n",
        "이 상황은 물체 검출 모델을 학습시키는 과정에서 문제가 될 수 있습니다. 대부분의 이미지에서는 물체보다 배경이 많습니다. 따라서 이미지는 극단적으로 배경의 class가 많은 class imbalanced data라고 할 수 있습니다. 이렇게 너무 많은 배경 class에 압도되지 않도록 modulating factor로 손실을 조절해줍니다.$\\gamma$를 0으로 설정하면 modulating factor $(1-p_t)^\\gamma$\n",
        " 가 1이 되어 일반적인 교차 엔트로피가 되고 $\\gamma$가 커질수록 modulating이 강하게 적용되는 것을 확인할 수 있습니다.\n",
        "\n",
        "\n",
        "\n",
        "### FPN(Feature Pyramid Network)\n",
        "> 여러 층의 특성 맵(feature map)을 다 사용해보자\n",
        "\n",
        "![img](https://d3s0tskafalll9.cloudfront.net/media/images/GC-4-P-fpn.max-800x600.png)\n",
        "https://arxiv.org/abs/1612.03144\n",
        "\n",
        "FPN은 특성을 피라미드처럼 쌓아서 사용하는 방식입니다. CNN 백본 네트워크에서는 다양한 레이어의 결과값을 특성 맵(feature map)으로 사용할 수 있습니다. 이때 컨볼루션 연산은 커널을 통해 일정한 영역을 보고 몇 개의 숫자로 요약해 내기 때문에, 입력 이미지를 기준으로 생각하면 입력 이미지와 먼 모델의 뒷쪽의 특성 맵일수록 하나의 \"셀(cell)\"이 넓은 이미지 영역의 정보를 담고 있고, 입력 이미지와 가까운 앞쪽 레이어의 특성 맵일수록 좁은 범위의 정보를 담고 있습니다. 이를 **receptive field**라고 합니다. 레이어가 깊어질 수록 pooling을 거쳐 넓은 범위의 정보(receptive field)를 갖게 되는 것입니다.\n",
        "\n",
        "\n",
        "FPN은 백본의 여러 레이어를 한꺼번에 쓰겠다라는데에 의의가 있습니다. SSD가 각 레이어의 특성 맵에서 다양한 크기에 대한 결과를 얻는 방식을 취했다면 RetinaNet에서는 receptive field가 넓은 뒷쪽의 특성 맵을 upsampling(확대)하여 앞단의 특성 맵과 더해서 사용했습니다. 레이어가 깊어질수록 feature map의 ww, hh방향의 receptive field가 넓어지는 것인데, 넓게 보는 것과 좁게 보는 것을 같이 쓰겠다는 목적인 거죠.\n",
        "\n",
        "![img](https://d3s0tskafalll9.cloudfront.net/media/images/GC-3-P-FPN.max-800x600.png)\n",
        "https://arxiv.org/abs/1708.02002\n",
        "\n",
        "위 그림은 RetinaNet 논문에서 FPN 구조가 어떻게 적용되었는지를 설명하는 그림입니다. RetinaNet에서는 FPN을 통해 $P_3$ 까지의 pyramid level을 생성해 사용합니다. 각 pyramid level은 256개의 채널로 이루어지게 됩니다. 이를 통해 **Classification Subnet**과 **Box Regression Subnet** 2개의 Subnet을 구성하게 되는데, Anchor 갯수를 $A$라고 하면 최종적으로 Classification Subnet은 $K$개 class에 대해 $KA$개 채널을, Box Regression Subnet은 $4A$개 채널을 사용하게 됩니다."
      ],
      "metadata": {
        "id": "s7-8C4n1pE6V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*****"
      ],
      "metadata": {
        "id": "Z9GviOCBI7No"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 준비\n",
        "### 데이터 파이프 라인\n",
        "먼저 주어진 KITTI 데이터를 학습에 맞는 형태로 바꾸어 주어야 합니다. 이때 사용할 데이터 파이프라인을 구축합니다.\n",
        "\n",
        "\n",
        "데이터 파이프라인은 총 4단계로 이루어집니다.\n",
        "\n",
        "\n",
        "1. x와 y좌표 위치 교체\n",
        "2. 무작위로 수평 뒤집기(Flip)\n",
        "3. 이미지 크기 조정 및 패딩 추가\n",
        "4. 좌표계를 [x_min, y_min, x_max, y_max]에서 [x_min, y_min, width, height]으로 수정\n",
        "\n",
        "\n",
        "독립적인 함수를 각각 작성합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "RyaE0fJ7I9ww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def swap_xy(boxes):\n",
        "    return tf.stack([boxes[:, 1], boxes[:, 0], boxes[:, 3], boxes[:, 2]], axis=-1)\n",
        "\n",
        "print('슝=3')"
      ],
      "metadata": {
        "id": "ipCyq4HwEq2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_flip_horizontal(image, boxes):\n",
        "    if tf.random.uniform(()) > 0.5:\n",
        "        image = tf.image.flip_left_right(image)\n",
        "        boxes = tf.stack(\n",
        "           [1 - boxes[:, 2], boxes[:, 1], 1 - boxes[:, 0], boxes[:, 3]], axis=-1\n",
        "        )\n",
        "        \n",
        "    return image, boxes\n",
        "\n",
        "print('슝=3')"
      ],
      "metadata": {
        "id": "ZTFY6GOEJHtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이미지 크기를 바꿀 때는 고려할 점이 있습니다. 이미지의 비율은 그대로 유지되어야 하고, 이미지의 최대/최소 크기도 제한해야 하거든요. 또 이미지의 크기를 바꾼 후에도 최종적으로 모델에 입력되는 이미지의 크기는 stride의 배수가 되도록 만들 거예요.\n",
        "\n",
        "예를 들어 600x720 크기의 이미지가 있다면 800x960 크기로 바꿀 수 있습니다. 여기에 stride를 128로 놓아 800x960 크기의 이미지에 패딩을 더해 896x1024 크기의 이미지로 모델에 입력하겠다는 이야깁니다. 모델에 입력되는 이미지에는 검정 테두리가 있겠군요!\n",
        "\n",
        "실제로 입력할 이미지를 어떻게 바꿀지는 min_side, max_side, min_side_range, stride등에 의해 결정돼요. 그리고 학습이 완료된 모델을 사용할 때는 입력할 이미지를 다양한 크기로 바꿀 필요는 없으니 분기처리를 해줍니다."
      ],
      "metadata": {
        "id": "M8eFa_xvJI2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_and_pad_image(image, training=True):\n",
        "\n",
        "    min_side = 800.0\n",
        "    max_side = 1333.0\n",
        "    min_side_range = [640, 1024]\n",
        "    stride = 128.0\n",
        "    \n",
        "    image_shape = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n",
        "    if training:\n",
        "        min_side = tf.random.uniform((), min_side_range[0], min_side_range[1], dtype=tf.float32)\n",
        "    ratio = min_side / tf.reduce_min(image_shape)\n",
        "    if ratio * tf.reduce_max(image_shape) > max_side:\n",
        "        ratio = max_side / tf.reduce_max(image_shape)\n",
        "    image_shape = ratio * image_shape\n",
        "    image = tf.image.resize(image, tf.cast(image_shape, dtype=tf.int32))\n",
        "    padded_image_shape = tf.cast(\n",
        "        tf.math.ceil(image_shape / stride) * stride, dtype=tf.int32\n",
        "    )\n",
        "    image = tf.image.pad_to_bounding_box(\n",
        "        image, 0, 0, padded_image_shape[0], padded_image_shape[1]\n",
        "    )\n",
        "    return image, image_shape, ratio\n",
        "\n",
        "print('슝=3')"
      ],
      "metadata": {
        "id": "H8r41LJ5JKGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_xywh(boxes):\n",
        "    return tf.concat(\n",
        "        [(boxes[..., :2] + boxes[..., 2:]) / 2.0, boxes[..., 2:] - boxes[..., :2]],\n",
        "        axis=-1,\n",
        "    )\n",
        "\n",
        "print('슝=3')"
      ],
      "metadata": {
        "id": "MWgQeOy3JRoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 준비된 함수들을 연결해 줍니다."
      ],
      "metadata": {
        "id": "zLGSBwKwJP0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(sample):\n",
        "    image = sample[\"image\"]\n",
        "    bbox = swap_xy(sample[\"objects\"][\"bbox\"])\n",
        "    class_id = tf.cast(sample[\"objects\"][\"type\"], dtype=tf.int32)\n",
        "\n",
        "    image, bbox = random_flip_horizontal(image, bbox)\n",
        "    image, image_shape, _ = resize_and_pad_image(image)\n",
        "\n",
        "    bbox = tf.stack(\n",
        "        [\n",
        "            bbox[:, 0] * image_shape[1],\n",
        "            bbox[:, 1] * image_shape[0],\n",
        "            bbox[:, 2] * image_shape[1],\n",
        "            bbox[:, 3] * image_shape[0],\n",
        "        ],\n",
        "        axis=-1,\n",
        "    )\n",
        "    bbox = convert_to_xywh(bbox)\n",
        "    return image, bbox, class_id\n",
        "\n",
        "print('슝=3')"
      ],
      "metadata": {
        "id": "EZJIwLqNJSmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 인코딩\n",
        "One stage detector에서는 Anchor Box라는 정해져 있는 위치, 크기, 비율 중에 하나로 물체의 위치가 결정됩니다. 그래서 기본적으로 Anchor Box를 생성해줘야 합니다. Anchor Box로 생성되는 것은 물체 위치 후보라고 생각하면 됩니다. 물체 위치를 주관식이 아닌 객관식으로 풀게 하는 겁니다.\n",
        "\n",
        "예를 들어 100개의 Anchor Box를 생성했다고 가정하면 이미 만들어진 100개의 Anchor Box에 해당하지 않는 위치, 크기, 비율에 물체가 있을 수 없습니다. 100개의 Anchor Box중 가장 근접한 하나가 선택이 되겠죠. 이렇게 선택된 Anchor Box를 기초로 정확한 위치를 찾아냅니다. 추가로 Anchor Box로부터 상하좌우로 떨어진 정도, 가로 세로의 크기 차이를 미세하게 찾아내죠. 게다가 Anchor Box가 촘촘하게 겹치도록 생성되기 때문에 물체를 잘 찾아낼 수 있습니다.\n",
        "\n",
        "또, RetinaNet에서는 FPN을 사용하기 때문에 Anchor Box가 더 많이 필요합니다. FPN의 각 층마다 Anchor Box가 필요하기 때문입니다. RetinaNet의 FPN에서 pyramid level은 개수가 미리 약속되어 있기 때문에 각 level에서 만들어지는 Anchor Box도 약속되어 있습니다.\n",
        "\n",
        "여기서는 논문과 같은 형태로 Anchor Box를 생성합니다.\n",
        "\n",
        "![img](https://d3s0tskafalll9.cloudfront.net/media/original_images/gc-4v2-p-4-1.png)\n",
        "https://arxiv.org/pdf/1708.02002.pdf"
      ],
      "metadata": {
        "id": "xnW7TkEQJT5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AnchorBox:\n",
        "    def __init__(self):\n",
        "        self.aspect_ratios = [0.5, 1.0, 2.0]\n",
        "        self.scales = [2 ** x for x in [0, 1 / 3, 2 / 3]]\n",
        "\n",
        "        self._num_anchors = len(self.aspect_ratios) * len(self.scales)\n",
        "        self._strides = [2 ** i for i in range(3, 8)]\n",
        "        self._areas = [x ** 2 for x in [32.0, 64.0, 128.0, 256.0, 512.0]]\n",
        "        self._anchor_dims = self._compute_dims()\n",
        "\n",
        "    def _compute_dims(self):\n",
        "        anchor_dims_all = []\n",
        "        for area in self._areas:\n",
        "            anchor_dims = []\n",
        "            for ratio in self.aspect_ratios:\n",
        "                anchor_height = tf.math.sqrt(area / ratio)\n",
        "                anchor_width = area / anchor_height\n",
        "                dims = tf.reshape(\n",
        "                    tf.stack([anchor_width, anchor_height], axis=-1), [1, 1, 2]\n",
        "                )\n",
        "                for scale in self.scales:\n",
        "                    anchor_dims.append(scale * dims)\n",
        "            anchor_dims_all.append(tf.stack(anchor_dims, axis=-2))\n",
        "        return anchor_dims_all\n",
        "\n",
        "    def _get_anchors(self, feature_height, feature_width, level):\n",
        "        rx = tf.range(feature_width, dtype=tf.float32) + 0.5\n",
        "        ry = tf.range(feature_height, dtype=tf.float32) + 0.5\n",
        "        centers = tf.stack(tf.meshgrid(rx, ry), axis=-1) * self._strides[level - 3]\n",
        "        centers = tf.expand_dims(centers, axis=-2)\n",
        "        centers = tf.tile(centers, [1, 1, self._num_anchors, 1])\n",
        "        dims = tf.tile(\n",
        "            self._anchor_dims[level - 3], [feature_height, feature_width, 1, 1]\n",
        "        )\n",
        "        anchors = tf.concat([centers, dims], axis=-1)\n",
        "        return tf.reshape(\n",
        "            anchors, [feature_height * feature_width * self._num_anchors, 4]\n",
        "        )\n",
        "\n",
        "    def get_anchors(self, image_height, image_width):\n",
        "        anchors = [\n",
        "            self._get_anchors(\n",
        "                tf.math.ceil(image_height / 2 ** i),\n",
        "                tf.math.ceil(image_width / 2 ** i),\n",
        "                i,\n",
        "            )\n",
        "            for i in range(3, 8)\n",
        "        ]\n",
        "        return tf.concat(anchors, axis=0)\n",
        "\n",
        "print('슝=3')"
      ],
      "metadata": {
        "id": "QA91VeWmJc1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 Anchor Box를 생성했으니 입력할 데이터를 Anchor Box에 맞게 변형해줘야 합니다.\n",
        "\n",
        "데이터 원본의 bbox는 주관식 정답이라고 생각하면 됩니다. 하지만 모델은 객관식으로 문제를 풀어야 하기 때문에 주관식 정답을 가장 가까운 객관식 정답으로 바꿔줘야 모델을 학습시킬 수 있습니다.\n",
        "\n",
        "그럼 어떻게 주관식 정답을 객관식 정답으로 바꿀 수 있을까요? 여기에서 IoU를 사용합니다.\n",
        "\n",
        "IoU가 높은지 낮은지에 따라 Anchor Box가 정답인지 오답인지 체크해 두는 것이죠. 그러니 IoU를 계산할 수 있는 함수를 만듭니다."
      ],
      "metadata": {
        "id": "JicgHa4JJe0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_corners(boxes):\n",
        "    return tf.concat(\n",
        "        [boxes[..., :2] - boxes[..., 2:] / 2.0, boxes[..., :2] + boxes[..., 2:] / 2.0],\n",
        "        axis=-1,\n",
        "    )\n",
        "\n",
        "def compute_iou(boxes1, boxes2):\n",
        "    boxes1_corners = convert_to_corners(boxes1)\n",
        "    boxes2_corners = convert_to_corners(boxes2)\n",
        "    lu = tf.maximum(boxes1_corners[:, None, :2], boxes2_corners[:, :2])\n",
        "    rd = tf.minimum(boxes1_corners[:, None, 2:], boxes2_corners[:, 2:])\n",
        "    intersection = tf.maximum(0.0, rd - lu)\n",
        "    intersection_area = intersection[:, :, 0] * intersection[:, :, 1]\n",
        "    boxes1_area = boxes1[:, 2] * boxes1[:, 3]\n",
        "    boxes2_area = boxes2[:, 2] * boxes2[:, 3]\n",
        "    union_area = tf.maximum(\n",
        "        boxes1_area[:, None] + boxes2_area - intersection_area, 1e-8\n",
        "    )\n",
        "    return tf.clip_by_value(intersection_area / union_area, 0.0, 1.0)\n",
        "\n",
        "print('슝=3')"
      ],
      "metadata": {
        "id": "AlkyyRV-Jf1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 실제 라벨을 Anchor Box에 맞춰주는 클래스를 만들어 봅시다. 위에서 작성한 compute_iou 함수를 이용해서 IoU를 구하고 그 IoU를 기준으로 물체에 해당하는 Anchor Box와 배경이 되는 Anchor Box를 지정해 줍니다. 그리고 그 Anchor Box와 실제 Bounding Box의 미세한 차이를 계산합니다. 상하좌우의 차이, 가로세로 크기의 차이를 기록해 두는데 가로세로 크기는 로그를 사용해서 기록해 둡니다.\n",
        "\n",
        "> 이 과정에서 `variance`가 등장하는데 관례적으로 Anchor Box를 사용할 때 등장합니다.\n",
        "어디에도 정확한 이유가 등장하지는 않지만 상하좌우의 차이에는 0.1, 가로세로 크기의 차이에는 0.2를 사용합니다.\n",
        "이와 관련하여 통계적 추정치를 계산할 때 분산으로 나눠주는 것 때문이라는 의견이 있습니다.\n",
        "\n",
        "이 과정은 마치 데이터를 훈련이 가능한 형식으로 encode하는 것 같으니 LabelEncoder라는 이름으로 클래스를 만들었습니다.\n",
        "\n",
        "![img](https://d3s0tskafalll9.cloudfront.net/media/original_images/gc-4v2-p-4-2.png)\n",
        "[IoU가 0.5보다 높으면 물체, 0.4보다 낮으면 배경입니다]\n",
        "https://arxiv.org/pdf/1708.02002.pdf\n"
      ],
      "metadata": {
        "id": "BtWcpSofJhKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LabelEncoder:\n",
        "\n",
        "    def __init__(self):\n",
        "        self._anchor_box = AnchorBox()\n",
        "        self._box_variance = tf.convert_to_tensor(\n",
        "            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\n",
        "        )\n",
        "\n",
        "    def _match_anchor_boxes(\n",
        "        self, anchor_boxes, gt_boxes, match_iou=0.5, ignore_iou=0.4\n",
        "    ):\n",
        "        iou_matrix = compute_iou(anchor_boxes, gt_boxes)\n",
        "        max_iou = tf.reduce_max(iou_matrix, axis=1)\n",
        "        matched_gt_idx = tf.argmax(iou_matrix, axis=1)\n",
        "        positive_mask = tf.greater_equal(max_iou, match_iou)\n",
        "        negative_mask = tf.less(max_iou, ignore_iou)\n",
        "        ignore_mask = tf.logical_not(tf.logical_or(positive_mask, negative_mask))\n",
        "        return (\n",
        "            matched_gt_idx,\n",
        "            tf.cast(positive_mask, dtype=tf.float32),\n",
        "            tf.cast(ignore_mask, dtype=tf.float32),\n",
        "        )\n",
        "\n",
        "    def _compute_box_target(self, anchor_boxes, matched_gt_boxes):\n",
        "        box_target = tf.concat(\n",
        "            [\n",
        "                (matched_gt_boxes[:, :2] - anchor_boxes[:, :2]) / anchor_boxes[:, 2:],\n",
        "                tf.math.log(matched_gt_boxes[:, 2:] / anchor_boxes[:, 2:]),\n",
        "            ],\n",
        "            axis=-1,\n",
        "        )\n",
        "        box_target = box_target / self._box_variance\n",
        "        return box_target\n",
        "\n",
        "    def _encode_sample(self, image_shape, gt_boxes, cls_ids):\n",
        "        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
        "        cls_ids = tf.cast(cls_ids, dtype=tf.float32)\n",
        "        matched_gt_idx, positive_mask, ignore_mask = self._match_anchor_boxes(\n",
        "            anchor_boxes, gt_boxes\n",
        "        )\n",
        "        matched_gt_boxes = tf.gather(gt_boxes, matched_gt_idx)\n",
        "        box_target = self._compute_box_target(anchor_boxes, matched_gt_boxes)\n",
        "        matched_gt_cls_ids = tf.gather(cls_ids, matched_gt_idx)\n",
        "        cls_target = tf.where(\n",
        "            tf.not_equal(positive_mask, 1.0), -1.0, matched_gt_cls_ids\n",
        "        )\n",
        "        cls_target = tf.where(tf.equal(ignore_mask, 1.0), -2.0, cls_target)\n",
        "        cls_target = tf.expand_dims(cls_target, axis=-1)\n",
        "        label = tf.concat([box_target, cls_target], axis=-1)\n",
        "        return label\n",
        "\n",
        "    def encode_batch(self, batch_images, gt_boxes, cls_ids):\n",
        "        images_shape = tf.shape(batch_images)\n",
        "        batch_size = images_shape[0]\n",
        "\n",
        "        labels = tf.TensorArray(dtype=tf.float32, size=batch_size, dynamic_size=True)\n",
        "        for i in range(batch_size):\n",
        "            label = self._encode_sample(images_shape, gt_boxes[i], cls_ids[i])\n",
        "            labels = labels.write(i, label)\n",
        "        batch_images = tf.keras.applications.resnet.preprocess_input(batch_images)\n",
        "        return batch_images, labels.stack()\n",
        "    \n",
        "print('슝=3')"
      ],
      "metadata": {
        "id": "NPryZ4ydJ0_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 데이터를 모델이 학습 가능한 형태로 바꿔 줄 수 있게 되었으니 모델을 만들러 가봅시다."
      ],
      "metadata": {
        "id": "jRppq0VWJ89E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 작성\n",
        "### Feature Pyramid\n",
        "앞서 설명했듯이 RetinaNet에서는 FPN(Feature Pyramid Network)를 사용합니다. 완전히 동일한 것은 아니고 약간 수정해서 사용했습니다. 자세한 설명은 아래에 나와있네요.\n",
        "\n",
        "![img](https://d3s0tskafalll9.cloudfront.net/media/original_images/gc-4v2-p-5-1.png)\n",
        "[FPN을 약간 수정합니다]\n",
        "https://arxiv.org/pdf/1708.02002.pdf"
      ],
      "metadata": {
        "id": "zn9AY2gvJ-Rv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeaturePyramid(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, backbone):\n",
        "        super(FeaturePyramid, self).__init__(name=\"FeaturePyramid\")\n",
        "        self.backbone = backbone\n",
        "        self.conv_c3_1x1 = tf.keras.layers.Conv2D(256, 1, 1, \"same\")\n",
        "        self.conv_c4_1x1 = tf.keras.layers.Conv2D(256, 1, 1, \"same\")\n",
        "        self.conv_c5_1x1 = tf.keras.layers.Conv2D(256, 1, 1, \"same\")\n",
        "        self.conv_c3_3x3 = tf.keras.layers.Conv2D(256, 3, 1, \"same\")\n",
        "        self.conv_c4_3x3 = tf.keras.layers.Conv2D(256, 3, 1, \"same\")\n",
        "        self.conv_c5_3x3 = tf.keras.layers.Conv2D(256, 3, 1, \"same\")\n",
        "        self.conv_c6_3x3 = tf.keras.layers.Conv2D(256, 3, 2, \"same\")\n",
        "        self.conv_c7_3x3 = tf.keras.layers.Conv2D(256, 3, 2, \"same\")\n",
        "        self.upsample_2x = tf.keras.layers.UpSampling2D(2)\n",
        "\n",
        "    def call(self, images, training=False):\n",
        "        c3_output, c4_output, c5_output = self.backbone(images, training=training)\n",
        "        p3_output = self.conv_c3_1x1(c3_output)\n",
        "        p4_output = self.conv_c4_1x1(c4_output)\n",
        "        p5_output = self.conv_c5_1x1(c5_output)\n",
        "        p4_output = p4_output + self.upsample_2x(p5_output)\n",
        "        p3_output = p3_output + self.upsample_2x(p4_output)\n",
        "        p3_output = self.conv_c3_3x3(p3_output)\n",
        "        p4_output = self.conv_c4_3x3(p4_output)\n",
        "        p5_output = self.conv_c5_3x3(p5_output)\n",
        "        p6_output = self.conv_c6_3x3(c5_output)\n",
        "        p7_output = self.conv_c7_3x3(tf.nn.relu(p6_output))\n",
        "        return p3_output, p4_output, p5_output, p6_output, p7_output\n",
        "\n",
        "print('슝=3')"
      ],
      "metadata": {
        "id": "RPr1TMB2KGB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Object Detection의 라벨은 class와 box로 이루어지므로 각각을 추론하는 부분이 필요합니다. 그것을 head라고 부르기도 합니다. Backbone에 해당하는 네트워크와 FPN을 통해 pyramid layer가 추출되고 나면 그 feature들을 바탕으로 class를 예상하고, box도 예상합니다. class와 box가 모두 맞을 수도, class와 box 중 하나만 맞을 수도, 둘 다 틀릴 수도 있겠죠? class를 예측하는 head와 box를 예측하는 head가 별도로 존재한다는 것이 중요합니다.\n",
        "\n",
        "그래서 각각의 head를 만들어 줍니다. head부분은 유사한 형태로 만들 수 있으니 build_head라는 함수를 하나만 만들고 두 번 호출하면 될 것 같네요."
      ],
      "metadata": {
        "id": "jwcayVuNKHZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_head(output_filters, bias_init):\n",
        "    head = tf.keras.Sequential([tf.keras.Input(shape=[None, None, 256])])\n",
        "    kernel_init = tf.initializers.RandomNormal(0.0, 0.01)\n",
        "    for _ in range(4):\n",
        "        head.add(\n",
        "            tf.keras.layers.Conv2D(256, 3, padding=\"same\", kernel_initializer=kernel_init)\n",
        "        )\n",
        "        head.add(tf.keras.layers.ReLU())\n",
        "    head.add(\n",
        "        tf.keras.layers.Conv2D(\n",
        "            output_filters,\n",
        "            3,\n",
        "            1,\n",
        "            padding=\"same\",\n",
        "            kernel_initializer=kernel_init,\n",
        "            bias_initializer=bias_init,\n",
        "        )\n",
        "    )\n",
        "    return head\n",
        "\n",
        "print('슝=3')"
      ],
      "metadata": {
        "id": "OmsVZuGvKI_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "우리가 만들 RetinaNet의 backbone은 ResNet50입니다. FPN에 이용할 수 있도록 중간 레이어도 output으로 연결해 줍니다."
      ],
      "metadata": {
        "id": "TLPokBYKKKIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_backbone():\n",
        "    backbone = tf.keras.applications.ResNet50(\n",
        "        include_top=False, input_shape=[None, None, 3]\n",
        "    )\n",
        "    c3_output, c4_output, c5_output = [\n",
        "        backbone.get_layer(layer_name).output\n",
        "        for layer_name in [\"conv3_block4_out\", \"conv4_block6_out\", \"conv5_block3_out\"]\n",
        "    ]\n",
        "    return tf.keras.Model(\n",
        "        inputs=[backbone.inputs], outputs=[c3_output, c4_output, c5_output]\n",
        "    )\n",
        "\n",
        "print('슝=3')"
      ],
      "metadata": {
        "id": "nfoiu6_AYcnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 RetinaNet을 완성해 봅시다. Backbone + FPN + classification용 head + box용 head 입니다."
      ],
      "metadata": {
        "id": "bLaicbn1KN1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RetinaNet(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, num_classes, backbone):\n",
        "        super(RetinaNet, self).__init__(name=\"RetinaNet\")\n",
        "        self.fpn = FeaturePyramid(backbone)\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        prior_probability = tf.constant_initializer(-np.log((1 - 0.01) / 0.01))\n",
        "        self.cls_head = build_head(9 * num_classes, prior_probability)\n",
        "        self.box_head = build_head(9 * 4, \"zeros\")\n",
        "\n",
        "    def call(self, image, training=False):\n",
        "        features = self.fpn(image, training=training)\n",
        "        N = tf.shape(image)[0]\n",
        "        cls_outputs = []\n",
        "        box_outputs = []\n",
        "        for feature in features:\n",
        "            box_outputs.append(tf.reshape(self.box_head(feature), [N, -1, 4]))\n",
        "            cls_outputs.append(\n",
        "                tf.reshape(self.cls_head(feature), [N, -1, self.num_classes])\n",
        "            )\n",
        "        cls_outputs = tf.concat(cls_outputs, axis=1)\n",
        "        box_outputs = tf.concat(box_outputs, axis=1)\n",
        "        return tf.concat([box_outputs, cls_outputs], axis=-1)\n",
        "\n",
        "print('슝=3')"
      ],
      "metadata": {
        "id": "k1U7IuV1KMCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 모델을 준비했고, Loss에 대한 준비를 해봅시다.\n",
        "\n",
        "RetinaNet에서는 Focal Loss를 사용하는데요. Box Regression에는 사용하지 않고 Classification Loss를 계산하는데만 사용됩니다. Box Regression에는 Smooth L1 Loss를 사용했네요.\n",
        "\n",
        "![img](https://d3s0tskafalll9.cloudfront.net/media/original_images/gc-4v2-p-5-2.png)\n",
        "[Focal Loss + Smooth L1 Loss]\n",
        "https://arxiv.org/pdf/1708.02002.pdf\n",
        "\n",
        "Smooth L1 Loss을 사용하는 Box Regression에는 delta를 기준으로 계산이 달라지고, Focal Loss를 사용하는 Classification에서는 alpha와 gamma를 사용해서 물체일 때와 배경일 때의 식이 달라지는 점에 주의하세요!"
      ],
      "metadata": {
        "id": "7duu3ygAKQbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RetinaNetBoxLoss(tf.losses.Loss):\n",
        "\n",
        "    def __init__(self, delta):\n",
        "        super(RetinaNetBoxLoss, self).__init__(\n",
        "            reduction=\"none\", name=\"RetinaNetBoxLoss\"\n",
        "        )\n",
        "        self._delta = delta\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        difference = y_true - y_pred\n",
        "        absolute_difference = tf.abs(difference)\n",
        "        squared_difference = difference ** 2\n",
        "        loss = tf.where(\n",
        "            tf.less(absolute_difference, self._delta),\n",
        "            0.5 * squared_difference,\n",
        "            absolute_difference - 0.5,\n",
        "        )\n",
        "        return tf.reduce_sum(loss, axis=-1)\n",
        "\n",
        "\n",
        "class RetinaNetClassificationLoss(tf.losses.Loss):\n",
        "\n",
        "    def __init__(self, alpha, gamma):\n",
        "        super(RetinaNetClassificationLoss, self).__init__(\n",
        "            reduction=\"none\", name=\"RetinaNetClassificationLoss\"\n",
        "        )\n",
        "        self._alpha = alpha\n",
        "        self._gamma = gamma\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            labels=y_true, logits=y_pred\n",
        "        )\n",
        "        probs = tf.nn.sigmoid(y_pred)\n",
        "        alpha = tf.where(tf.equal(y_true, 1.0), self._alpha, (1.0 - self._alpha))\n",
        "        pt = tf.where(tf.equal(y_true, 1.0), probs, 1 - probs)\n",
        "        loss = alpha * tf.pow(1.0 - pt, self._gamma) * cross_entropy\n",
        "        return tf.reduce_sum(loss, axis=-1)\n",
        "\n",
        "\n",
        "class RetinaNetLoss(tf.losses.Loss):\n",
        "\n",
        "    def __init__(self, num_classes=8, alpha=0.25, gamma=2.0, delta=1.0):\n",
        "        super(RetinaNetLoss, self).__init__(reduction=\"auto\", name=\"RetinaNetLoss\")\n",
        "        self._clf_loss = RetinaNetClassificationLoss(alpha, gamma)\n",
        "        self._box_loss = RetinaNetBoxLoss(delta)\n",
        "        self._num_classes = num_classes\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
        "        box_labels = y_true[:, :, :4]\n",
        "        box_predictions = y_pred[:, :, :4]\n",
        "        cls_labels = tf.one_hot(\n",
        "            tf.cast(y_true[:, :, 4], dtype=tf.int32),\n",
        "            depth=self._num_classes,\n",
        "            dtype=tf.float32,\n",
        "        )\n",
        "        cls_predictions = y_pred[:, :, 4:]\n",
        "        positive_mask = tf.cast(tf.greater(y_true[:, :, 4], -1.0), dtype=tf.float32)\n",
        "        ignore_mask = tf.cast(tf.equal(y_true[:, :, 4], -2.0), dtype=tf.float32)\n",
        "        clf_loss = self._clf_loss(cls_labels, cls_predictions)\n",
        "        box_loss = self._box_loss(box_labels, box_predictions)\n",
        "        clf_loss = tf.where(tf.equal(ignore_mask, 1.0), 0.0, clf_loss)\n",
        "        box_loss = tf.where(tf.equal(positive_mask, 1.0), box_loss, 0.0)\n",
        "        normalizer = tf.reduce_sum(positive_mask, axis=-1)\n",
        "        clf_loss = tf.math.divide_no_nan(tf.reduce_sum(clf_loss, axis=-1), normalizer)\n",
        "        box_loss = tf.math.divide_no_nan(tf.reduce_sum(box_loss, axis=-1), normalizer)\n",
        "        loss = clf_loss + box_loss\n",
        "        return loss\n",
        "\n",
        "print('슝=3')"
      ],
      "metadata": {
        "id": "xy6mtvuNKV6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 모든 준비가 끝났습니다. 모델 학습을 할 수 있겠네요!"
      ],
      "metadata": {
        "id": "RmkU6e8RKYui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 학습\n",
        "앞에서 만들어 놓은 클래스와 함수를 이용해서 모델을 조립하고 학습시켜 봅시다."
      ],
      "metadata": {
        "id": "1MbxPkM9KcXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 8\n",
        "batch_size = 2\n",
        "\n",
        "resnet50_backbone = get_backbone()\n",
        "loss_fn = RetinaNetLoss(num_classes)\n",
        "model = RetinaNet(num_classes, resnet50_backbone)\n",
        "\n",
        "print('슝=3')"
      ],
      "metadata": {
        "id": "12UQFnBKKapd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델은 매우 간단하게 만들어 졌네요.\n",
        "\n",
        "다음으로 Learning Rate입니다. 논문에서는 8개의 GPU를 사용했기 때문에 우리 환경과는 맞지 않아요. 그래서 Learning Rate를 적절히 바꿔줍니다.\n",
        "\n",
        "Optimizer는 동일하게 SGD를 사용합니다."
      ],
      "metadata": {
        "id": "n0t4_eT4KgkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rates = [2.5e-06, 0.000625, 0.00125, 0.0025, 0.00025, 2.5e-05]\n",
        "learning_rate_boundaries = [125, 250, 500, 240000, 360000]\n",
        "learning_rate_fn = tf.optimizers.schedules.PiecewiseConstantDecay(\n",
        "    boundaries=learning_rate_boundaries, values=learning_rates\n",
        ")\n",
        "optimizer = tf.optimizers.SGD(learning_rate=learning_rate_fn, momentum=0.9)\n",
        "model.compile(loss=loss_fn, optimizer=optimizer)"
      ],
      "metadata": {
        "id": "FglB0n-JKh04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 데이터 전처리를 위한 파이프라인도 만들어 줍니다."
      ],
      "metadata": {
        "id": "v9JMhh0LKkK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "(train_dataset, val_dataset), dataset_info = tfds.load(\n",
        "    \"kitti\", split=[\"train\", \"validation\"], with_info=True, data_dir=DATA_PATH\n",
        ")\n",
        "\n",
        "autotune = tf.data.AUTOTUNE\n",
        "train_dataset = train_dataset.map(preprocess_data, num_parallel_calls=autotune)\n",
        "train_dataset = train_dataset.shuffle(8 * batch_size)\n",
        "train_dataset = train_dataset.padded_batch(\n",
        "    batch_size=batch_size, padding_values=(0.0, 1e-8, -1), drop_remainder=True\n",
        ")\n",
        "train_dataset = train_dataset.map(\n",
        "    label_encoder.encode_batch, num_parallel_calls=autotune\n",
        ")\n",
        "train_dataset = train_dataset.prefetch(autotune)\n",
        "\n",
        "val_dataset = val_dataset.map(preprocess_data, num_parallel_calls=autotune)\n",
        "val_dataset = val_dataset.padded_batch(\n",
        "    batch_size=1, padding_values=(0.0, 1e-8, -1), drop_remainder=True\n",
        ")\n",
        "val_dataset = val_dataset.map(label_encoder.encode_batch, num_parallel_calls=autotune)\n",
        "val_dataset = val_dataset.prefetch(autotune)\n",
        "\n",
        "print('슝=3')"
      ],
      "metadata": {
        "id": "jWHHT1tPKlNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 학습을 시켜봅시다.\n",
        "\n",
        "1 epoch당 학습 시간이 상당히 오래 걸리기 때문에 여기서는 학습 시키는 예시 코드만 보여드릴게요. (뒤에선 미리 학습된 모델을 불러와 사용할 예정입니다.)\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "model_dir = os.getenv('HOME') + '/aiffel/object_detection/data/checkpoints/'\n",
        "callbacks_list = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=os.path.join(model_dir, \"weights\" + \"_epoch_{epoch}\"),\n",
        "        monitor=\"loss\",\n",
        "        save_best_only=False,\n",
        "        save_weights_only=True,\n",
        "        verbose=1,\n",
        "    )\n",
        "]\n",
        "\n",
        "epochs = 1\n",
        "\n",
        "model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks_list\n",
        ")\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "IsbtOTrwKmja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 결과 확인하기\n",
        "학습된 모델을 불러옵시다."
      ],
      "metadata": {
        "id": "8T1nclL3KvF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_dir = '/content/drive/MyDrive/object_detection/data/checkpoints'\n",
        "latest_checkpoint = tf.train.latest_checkpoint(model_dir)\n",
        "model.load_weights(latest_checkpoint)\n",
        "\n",
        "print('슝=3')"
      ],
      "metadata": {
        "id": "qsIeFTNNKyRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델의 추론 결과를 처리할 함수를 레이어 형식으로 만들어 줍니다. 논문에서는 1000개의 후보를 골라 처리했지만 우리는 100개의 후보만 골라 처리하도록 합시다. 나머지 설정은 논문과 동일하게 해볼게요.\n",
        "\n",
        "![img](https://d3s0tskafalll9.cloudfront.net/media/original_images/gc-4v2-p-7-1.png)\n",
        "[0.05보다 높은 box 1000개를 골라 0.5 NMS를 진행합니다]\n",
        "https://arxiv.org/pdf/1708.02002.pdf\n",
        "\n",
        "NMS(Non-Max Suppression)은 직접 구현하지 않고 주어진 tf.image.combined_non_max_suppression를 사용했습니다.\n",
        "\n",
        "* [tf.image.combined_non_max_suppression](https://www.tensorflow.org/api_docs/python/tf/image/combined_non_max_suppression)\n",
        "\n",
        "위 참고자료를 꼭 읽어보세요. 입출력되는 값이 어떤지 알아야 코드가 이해가 됩니다. 특히 출력에 nmsed_boxes, nmsed_scores, nmsed_classes, valid_detections이 각각 무엇인지 알아야 활용할 수 있습니다."
      ],
      "metadata": {
        "id": "pOTTR9-zKzes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecodePredictions(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes=8,\n",
        "        confidence_threshold=0.05,\n",
        "        nms_iou_threshold=0.5,\n",
        "        max_detections_per_class=100,\n",
        "        max_detections=100,\n",
        "        box_variance=[0.1, 0.1, 0.2, 0.2]\n",
        "    ):\n",
        "        super(DecodePredictions, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.confidence_threshold = confidence_threshold\n",
        "        self.nms_iou_threshold = nms_iou_threshold\n",
        "        self.max_detections_per_class = max_detections_per_class\n",
        "        self.max_detections = max_detections\n",
        "\n",
        "        self._anchor_box = AnchorBox()\n",
        "        self._box_variance = tf.convert_to_tensor(\n",
        "            box_variance, dtype=tf.float32\n",
        "        )\n",
        "\n",
        "    def _decode_box_predictions(self, anchor_boxes, box_predictions):\n",
        "        boxes = box_predictions * self._box_variance\n",
        "        boxes = tf.concat(\n",
        "            [\n",
        "                boxes[:, :, :2] * anchor_boxes[:, :, 2:] + anchor_boxes[:, :, :2],\n",
        "                tf.math.exp(boxes[:, :, 2:]) * anchor_boxes[:, :, 2:],\n",
        "            ],\n",
        "            axis=-1,\n",
        "        )\n",
        "        boxes_transformed = convert_to_corners(boxes)\n",
        "        return boxes_transformed\n",
        "\n",
        "    def call(self, images, predictions):\n",
        "        image_shape = tf.cast(tf.shape(images), dtype=tf.float32)\n",
        "        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
        "        box_predictions = predictions[:, :, :4]\n",
        "        cls_predictions = tf.nn.sigmoid(predictions[:, :, 4:])\n",
        "        boxes = self._decode_box_predictions(anchor_boxes[None, ...], box_predictions)\n",
        "\n",
        "        return tf.image.combined_non_max_suppression(\n",
        "            tf.expand_dims(boxes, axis=2),\n",
        "            cls_predictions,\n",
        "            self.max_detections_per_class,\n",
        "            self.max_detections,\n",
        "            self.nms_iou_threshold,\n",
        "            self.confidence_threshold,\n",
        "            clip_boxes=False,\n",
        "        )\n",
        "\n",
        "print('슝=3')"
      ],
      "metadata": {
        "id": "V-PwG9_oK-7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 추론이 가능한 모델을 조립합니다."
      ],
      "metadata": {
        "id": "ap1Ae_c_LAVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = tf.keras.Input(shape=[None, None, 3], name=\"image\")\n",
        "predictions = model(image, training=False)\n",
        "detections = DecodePredictions(confidence_threshold=0.5)(image, predictions)\n",
        "inference_model = tf.keras.Model(inputs=image, outputs=detections)\n",
        "\n",
        "print('슝=3')"
      ],
      "metadata": {
        "id": "mlwtVFDxLBcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델의 추론 결과를 시각화 할 함수를 만들어 줍니다."
      ],
      "metadata": {
        "id": "8NVxVFYvLCpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_detections(\n",
        "    image, boxes, classes, scores, figsize=(7, 7), linewidth=1, color=[0, 0, 1]\n",
        "):\n",
        "    image = np.array(image, dtype=np.uint8)\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(image)\n",
        "    ax = plt.gca()\n",
        "    for box, _cls, score in zip(boxes, classes, scores):\n",
        "        text = \"{}: {:.2f}\".format(_cls, score)\n",
        "        x1, y1, x2, y2 = box\n",
        "        w, h = x2 - x1, y2 - y1\n",
        "        patch = plt.Rectangle(\n",
        "            [x1, y1], w, h, fill=False, edgecolor=color, linewidth=linewidth\n",
        "        )\n",
        "        ax.add_patch(patch)\n",
        "        ax.text(\n",
        "            x1,\n",
        "            y1,\n",
        "            text,\n",
        "            bbox={\"facecolor\": color, \"alpha\": 0.4},\n",
        "            clip_box=ax.clipbox,\n",
        "            clip_on=True,\n",
        "        )\n",
        "    plt.show()\n",
        "    return ax\n",
        "\n",
        "print('슝=3')"
      ],
      "metadata": {
        "id": "kqFWjHroLD0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 추론시에 입력 데이터를 전처리하기 위한 함수를 만들게요.\n",
        "\n",
        "학습을 위한 전처리와 추론을 위한 전처리가 다르기 때문에 따로 작성됩니다. 추론을 위한 전처리가 훨씬 간단하네요."
      ],
      "metadata": {
        "id": "mNyuORpTLF4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_image(image):\n",
        "    image, _, ratio = resize_and_pad_image(image, training=False)\n",
        "    image = tf.keras.applications.resnet.preprocess_input(image)\n",
        "    return tf.expand_dims(image, axis=0), ratio\n",
        "\n",
        "print('슝=3')"
      ],
      "metadata": {
        "id": "sWdjOTBlLINl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 모든 것이 준비 되었으니 학습된 결과를 확인합시다!!"
      ],
      "metadata": {
        "id": "_00-Mhr3LJcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = tfds.load(\"kitti\", split=\"test\", data_dir=DATA_PATH)\n",
        "int2str = dataset_info.features[\"objects\"][\"type\"].int2str\n",
        "\n",
        "for sample in test_dataset.take(2):\n",
        "    image = tf.cast(sample[\"image\"], dtype=tf.float32)\n",
        "    input_image, ratio = prepare_image(image)\n",
        "    detections = inference_model.predict(input_image)\n",
        "    num_detections = detections.valid_detections[0]\n",
        "    class_names = [\n",
        "        int2str(int(x)) for x in detections.nmsed_classes[0][:num_detections]\n",
        "    ]\n",
        "    visualize_detections(\n",
        "        image,\n",
        "        detections.nmsed_boxes[0][:num_detections] / ratio,\n",
        "        class_names,\n",
        "        detections.nmsed_scores[0][:num_detections],\n",
        "    )"
      ],
      "metadata": {
        "id": "ZrwVD66ILKfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*****"
      ],
      "metadata": {
        "id": "F1yifHn-FISm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RetinaNet\n",
        "### 데이터 포맷 변경\n",
        "#### 클래스 및 바운딩 박스 정보 추출"
      ],
      "metadata": {
        "id": "f0eZj8eDgAzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "data_dir = '/content/drive/MyDrive/kitti'\n",
        "img_dir = data_dir + '/kitti_images'\n",
        "train_csv_path = data_dir + '/kitti_train.csv'\n",
        "\n",
        "# KITTI 데이터셋 ds_train을 파싱해서 dataframe으로 변환하는 parse_dataset 함수를 구현해 주세요.\n",
        "def parse_dataset(dataset, img_dir, total=0):\n",
        "    if not os.path.exists(img_dir):\n",
        "        os.mkdir(img_dir)\n",
        "    # Dataset의 claas를 확인하여 class에 따른 index를 확인해둡니다.\n",
        "    # 저는 기존의 class를 차와 사람으로 나누었습니다.\n",
        "    type_class_map = {\n",
        "        0: \"car\",\n",
        "        1: \"car\",\n",
        "        2: \"car\",\n",
        "        3: \"person\",\n",
        "        4: \"person\",\n",
        "        5: \"person\",\n",
        "    }\n",
        "    # Keras retinanet을 학습하기 위한 dataset을 csv로 parsing하기 위해서 필요한 column을 가진 pandas.DataFrame을 생성합니다.\n",
        "    df = pd.DataFrame(columns=[\"img_path\", \"x1\", \"y1\", \"x2\", \"y2\", \"class_name\"])\n",
        "    for item in tqdm(dataset, total=total):\n",
        "        filename = item['image/file_name'].numpy().decode('utf-8')\n",
        "        img_path = os.path.join(img_dir, filename)\n",
        "        \n",
        "        img = Image.fromarray(item['image'].numpy())\n",
        "        img.save(img_path)\n",
        "        object_bbox = item['objects']['bbox']\n",
        "        object_type = item['objects']['type'].numpy()\n",
        "        width, height = img.size\n",
        "        \n",
        "        # tf.dataset의 bbox 좌표가 0과 1 사이로 normalize된 좌표이므로 이를 pixel 좌표로 변환합니다.\n",
        "        x_min = object_bbox[:, 1] * width\n",
        "        x_max = object_bbox[:, 3] * width\n",
        "        y_min = height - object_bbox[:, 2] * height\n",
        "        y_max = height - object_bbox[:, 0] * height\n",
        "        \n",
        "        # 한 이미지에 있는 여러 Object들을 한 줄씩 pandas.DataFrame에 append합니다.\n",
        "        rects = np.stack([x_min, y_min, x_max, y_max], axis=1).astype(np.int)\n",
        "        for i, _rect in enumerate(rects):\n",
        "            _type = object_type[i]\n",
        "            if _type not in type_class_map.keys():\n",
        "                continue\n",
        "            df = df.append({\n",
        "                \"img_path\":img_path,\n",
        "                \"x1\": _rect[0],\n",
        "                \"y1\": _rect[1],\n",
        "                \"x2\": _rect[2],\n",
        "                \"y2\": _rect[3],\n",
        "                \"class_name\": type_class_map[_type]\n",
        "            }, ignore_index=True)\n",
        "    \n",
        "    return df\n",
        "\n",
        "df_train = parse_dataset(ds_train, img_dir, total=ds_info.splits['train'].num_examples)\n",
        "df_train.to_csv(train_csv_path, sep=',', index=False, header=False)"
      ],
      "metadata": {
        "id": "iMBwbDBCf8EE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "테스트 데이터셋에 대해서도 동일하게 parse_dataset() 을 적용해 dataframe 생성"
      ],
      "metadata": {
        "id": "Vpblt_34gHv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_csv_path = data_dir + '/kitti_test.csv'\n",
        "\n",
        "df_test = parse_dataset(ds_test, img_dir, total=ds_info.splits['test'].num_examples)\n",
        "df_test.to_csv(test_csv_path, sep=',', index=False, header=False)"
      ],
      "metadata": {
        "id": "kG_V5ppQgJL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "클래스 맵핑"
      ],
      "metadata": {
        "id": "Dc-EYcLygKV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_txt_path = data_dir + '/classes.txt'\n",
        "\n",
        "def save_class_format(path):\n",
        "    class_type_map = {\n",
        "        \"car\" : 0,\n",
        "        \"person\": 1\n",
        "    }\n",
        "    with open(path, mode='w', encoding='utf-8') as f:\n",
        "        for k, v in class_type_map.items():\n",
        "            f.write(f\"{k},{v}\\n\")\n",
        "            \n",
        "save_class_format(class_txt_path)"
      ],
      "metadata": {
        "id": "W9eoVJLsgMmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "미리 설치해 둔 케라스 및 Keras RetinaNet 리포지토리 사용해 RetinaNet 훈련"
      ],
      "metadata": {
        "id": "a9kTy_9qgNgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-resnet"
      ],
      "metadata": {
        "id": "6Bz4Q1x_M78s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RetinaNet 훈련이 시작됩니다!! 10epoch 훈련에 1시간 이상 소요될 수 있습니다. \n",
        "!cd /content/drive/MyDrive/object_detection && keras-retinanet/keras_retinanet/bin/train.py --gpu 0 --multiprocessing --workers 4 --batch-size 2 --epochs 10 --steps 195 csv data/kitti_train.csv data/classes.txt\n",
        "print(\"Done\")"
      ],
      "metadata": {
        "id": "kAKsi_ebgRo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습된 모델을 추론을 위해 실행할 수 있는 케라스 모델로 변환"
      ],
      "metadata": {
        "id": "jTHqZKlDgTz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/drive/MyDrive/object_detection && /content/keras-retinanet/keras_retinanet/bin/convert_model.py snapshots/resnet50_csv_10.h5 snapshots/r"
      ],
      "metadata": {
        "id": "dVUUq6wkgVSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 시각화\n",
        "#### 변환한 모델을 load, 추론 및 시각화"
      ],
      "metadata": {
        "id": "uS_QRUS9gWkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# automatically reload modules when they have changed\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# import keras\n",
        "import keras\n",
        "\n",
        "# import keras_retinanet\n",
        "from keras_retinanet import models\n",
        "from keras_retinanet.models import load_model\n",
        "from keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image\n",
        "from keras_retinanet.utils.visualization import draw_box, draw_caption\n",
        "from keras_retinanet.utils.colors import label_color\n",
        "from keras_retinanet.utils.gpu import setup_gpu\n",
        "\n",
        "# import miscellaneous modules\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "gpu = '0'\n",
        "setup_gpu(gpu)\n",
        "\n",
        "dir_path = '/content/drive/MyDrive/object_detection'\n",
        "model_path = os.path.join(dir_path, 'snapshots', 'resnet50_csv_10_infer.h5')\n",
        "model = load_model(model_path, backbone_name='resnet50')"
      ],
      "metadata": {
        "id": "R9DYZmgTgaZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 결과 확인을 위한 함수"
      ],
      "metadata": {
        "id": "mT_FKcfIgdbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inference_on_image 함수를 구현해 주세요.\n",
        "def inference_on_image(model, img_path=\"./test_set/go_1.png\", visualize=True):\n",
        "    image = read_image_bgr(img_path)\n",
        "\n",
        "    # copy to draw on\n",
        "    draw = image.copy()\n",
        "    draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    color_map = {\n",
        "        0: (0, 0, 255), # blue\n",
        "        1: (255, 0, 0) # red\n",
        "    }\n",
        "\n",
        "    # preprocess image for network\n",
        "    image = preprocess_image(image)\n",
        "    image, scale = resize_image(image)\n",
        "\n",
        "    # process image\n",
        "    boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0))\n",
        "\n",
        "    # correct for image scale\n",
        "    boxes /= scale\n",
        "\n",
        "    # display images\n",
        "    if  visualize:\n",
        "        for box, score, label in zip(boxes[0], scores[0], labels[0]):\n",
        "            print(box)\n",
        "            if score < 0.5:\n",
        "                break\n",
        "            b = box.astype(int)\n",
        "            draw_box(draw, b, color=color_map[label])\n",
        "\n",
        "            caption = \"{:.3f}\".format(score)\n",
        "            draw_caption(draw, b, caption)\n",
        "\n",
        "        plt.figure(figsize=(15, 15))\n",
        "        plt.axis('off')\n",
        "        plt.imshow(draw)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "OPaD0A3xgfwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_path = '/content/drive/MyDrive/object_detection/test_set/go_3.png'\n",
        "print(inference_on_image(model, img_path=img_path))"
      ],
      "metadata": {
        "id": "gHvshW_YghKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 다른 이미지에서 확인"
      ],
      "metadata": {
        "id": "Lw0B3eY5gir1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_path = '/content/drive/MyDrive/object_detection/test_set/stop_1.png'\n",
        "inference_on_image(model, img_path)"
      ],
      "metadata": {
        "id": "AKgQydDogkkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 프로젝트: 자율주행 보조 시스템 만들기"
      ],
      "metadata": {
        "id": "-pn9kIRjFJde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 주요 라이브러리 버전 확인\n",
        "import tensorflow as tf\n",
        "\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "531BGpakFOAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 준비"
      ],
      "metadata": {
        "id": "pphZahLQcXyd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 자율주행 시스템 만들기\n",
        "위에서 만든 모델을 통해 아래의 조건을 만족하는 함수를 만들어 주세요.\n",
        "\n",
        "- 입력으로 이미지 경로를 받습니다.\n",
        "- 정지조건에 맞는 경우 \"Stop\" 아닌 경우 \"Go\"를 반환합니다.\n",
        "- 조건은 다음과 같습니다.\n",
        "  - 사람이 한 명 이상 있는 경우\n",
        "  - 차량의 크기(width or height)가 300px이상인 경우"
      ],
      "metadata": {
        "id": "7rDN6LIzFQPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_path = '/content/drive/MyDrive/object_detection/test_set/stop_1.png'\n",
        "\n",
        "def self_drive_assist(model, img_path, size_limit=300, visualize=True):\n",
        "    image = read_image_bgr(img_path)\n",
        "    \n",
        "    # copy to draw on\n",
        "    draw = image.copy()\n",
        "    draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    color_map = {\n",
        "        0: (0, 0, 255), # blue\n",
        "        1: (255, 0, 0) # red\n",
        "    }\n",
        "    \n",
        "    #preprocess image for network\n",
        "    image = preprocess_image(image)\n",
        "    image, scale = resize_image(image)\n",
        "    \n",
        "    #process image\n",
        "    boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0))\n",
        "    \n",
        "    # correct for image scale\n",
        "    boxes /= scale\n",
        "    \n",
        "    # display images\n",
        "    if visualize:\n",
        "        result = 'Go'\n",
        "        for box, score, label in zip(boxes[0], scores[0], labels[0]):\n",
        "            print(box)\n",
        "            if score < 0.5:\n",
        "                break\n",
        "            b = box.astype(int)\n",
        "            w = b[2]-b[0]\n",
        "            h = b[3]-b[1]\n",
        "            \n",
        "            if w >= size_limit or h >= size_limit or label == 1:\n",
        "                result = 'Stop'\n",
        "            else:\n",
        "                result = 'Go'\n",
        "                \n",
        "            draw_box(draw, b, color=color_map[label])\n",
        "            \n",
        "            caption = \"{:.3f}\".format(score)\n",
        "            draw_caption(draw, b, caption)\n",
        "        \n",
        "        plt.figure(figsize=(15, 15))\n",
        "        plt.axis('off')\n",
        "        plt.imshow(draw)\n",
        "        plt.show()\n",
        "        \n",
        "        return result"
      ],
      "metadata": {
        "id": "RXK7ZjooFXC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_path =  '/content/drive/MyDrive/object_detection/test_set/stop_2.png'\n",
        "self_drive_assist(model=model, img_path=img_path)"
      ],
      "metadata": {
        "id": "vQIumDJigu1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 자율주행 시스템 평가하기\n",
        "아래 test_system() 를 통해서 위에서 만든 함수를 평가해봅시다. 10장에 대해 Go와 Stop을 맞게 반환하는지 확인하고 100점 만점으로 평가해줍니다."
      ],
      "metadata": {
        "id": "j2PuBz_KFYGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def test_system(func):\n",
        "    work_dir = '/content/drive/MyDrive/object_detection/data'\n",
        "    score = 0\n",
        "    test_set=[\n",
        "        (\"stop_1.png\", \"Stop\"),\n",
        "        (\"stop_2.png\", \"Stop\"),\n",
        "        (\"stop_3.png\", \"Stop\"),\n",
        "        (\"stop_4.png\", \"Stop\"),\n",
        "        (\"stop_5.png\", \"Stop\"),\n",
        "        (\"go_1.png\", \"Go\"),\n",
        "        (\"go_2.png\", \"Go\"),\n",
        "        (\"go_3.png\", \"Go\"),\n",
        "        (\"go_4.png\", \"Go\"),\n",
        "        (\"go_5.png\", \"Go\"),\n",
        "    ]\n",
        "    \n",
        "    for image_file, answer in test_set:\n",
        "        image_path = work_dir + '/' + image_file\n",
        "        pred = func(image_path)\n",
        "        if pred == answer:\n",
        "            score += 10\n",
        "    print(f\"{score}점입니다.\")\n",
        "\n",
        "test_system(self_drive_assist)"
      ],
      "metadata": {
        "id": "qILwn2FHFZy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 회고\n"
      ],
      "metadata": {
        "id": "DWGydcoGgywr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "A7in86iXlX6R"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "[GD8]GO/STOP! - Object Detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}